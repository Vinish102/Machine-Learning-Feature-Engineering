{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a13411c",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad66ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1) What is a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e051b309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn machine learning, a parameter is a configuration variable that is internal to the model and whose value is estimated from the data. \\nParameters are essential for defining the model’s behavior and are adjusted during the training process to minimize the error between the predicted and actual outputs.\\nKey Points about Parameters:\\n\\n    Learned from Data: Parameters are learned during the training phase. The model adjusts these values to fit the data as closely as possible.\\n\\n    Examples:\\n        Weights and Biases: In linear regression, the slope (weight) and intercept (bias) are parameters.\\n        Neural Networks: Weights and biases in the layers of a neural network are parameters.\\n\\n    Contrast with Hyperparameters:\\n        Parameters are learned from the data.\\n        Hyperparameters are set before training and control the learning process (e.g., learning rate, number of trees in a random forest).\\n\\n    Optimization: Algorithms like gradient descent are used to optimize parameters by minimizing a loss function.\\n\\n    Impact on Model: Parameters directly affect the model’s predictions and performance. \\n    Properly tuned parameters lead to better generalization on unseen data.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In machine learning, a parameter is a configuration variable that is internal to the model and whose value is estimated from the data. \n",
    "Parameters are essential for defining the model’s behavior and are adjusted during the training process to minimize the error between the predicted and actual outputs.\n",
    "Key Points about Parameters:\n",
    "\n",
    "    Learned from Data: Parameters are learned during the training phase. The model adjusts these values to fit the data as closely as possible.\n",
    "\n",
    "    Examples:\n",
    "        Weights and Biases: In linear regression, the slope (weight) and intercept (bias) are parameters.\n",
    "        Neural Networks: Weights and biases in the layers of a neural network are parameters.\n",
    "\n",
    "    Contrast with Hyperparameters:\n",
    "        Parameters are learned from the data.\n",
    "        Hyperparameters are set before training and control the learning process (e.g., learning rate, number of trees in a random forest).\n",
    "\n",
    "    Optimization: Algorithms like gradient descent are used to optimize parameters by minimizing a loss function.\n",
    "\n",
    "    Impact on Model: Parameters directly affect the model’s predictions and performance. \n",
    "    Properly tuned parameters lead to better generalization on unseen data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b590f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2) What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a750a184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCorrelation refers to a statistical measure that describes the extent to which two variables are linearly related. \\nIt indicates whether an increase or decrease in one variable corresponds to an increase or decrease in another variable.\\nKey Points about Correlation:\\n\\n    Types of Correlation:\\n        Positive Correlation: Both variables increase or decrease together. For example, height and weight often have a positive correlation.\\n        Negative Correlation: One variable increases while the other decreases. For example, the speed of a car and the time taken to reach a destination.\\n        No Correlation: No linear relationship between the variables.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Correlation refers to a statistical measure that describes the extent to which two variables are linearly related. \n",
    "It indicates whether an increase or decrease in one variable corresponds to an increase or decrease in another variable.\n",
    "Key Points about Correlation:\n",
    "\n",
    "    Types of Correlation:\n",
    "        Positive Correlation: Both variables increase or decrease together. For example, height and weight often have a positive correlation.\n",
    "        Negative Correlation: One variable increases while the other decreases. For example, the speed of a car and the time taken to reach a destination.\n",
    "        No Correlation: No linear relationship between the variables.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6689950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3) Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88d7f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine learning is a subset of artificial intelligence that involves the development of algorithms and \\nstatistical models that enable computers to perform tasks without explicit instructions. \\nInstead, these models learn patterns and make decisions based on data.\\nMain Components of Machine Learning:\\n\\n    Data:\\n        Definition: The raw information used to train and evaluate models.\\n        Role: High-quality, relevant data is crucial for building effective models.\\n\\n    Features:\\n        Definition: Individual measurable properties or characteristics used as input to the model.\\n        Role: Features are used to make predictions. Feature selection and engineering are critical for model performance.\\n\\n    Model:\\n        Definition: A mathematical representation of a real-world process.\\n        Role: The model is trained on data to learn patterns and make predictions.\\n\\n    Algorithm:\\n        Definition: A procedure or formula for solving a problem.\\n        Role: Algorithms are used to train models by adjusting parameters to minimize error.\\n\\n    Training:\\n        Definition: The process of teaching a model to make predictions by exposing it to data.\\n        Role: During training, the model learns the relationship between input features and the target variable.\\n\\n    Evaluation:\\n        Definition: Assessing the model’s performance using metrics like accuracy, precision, recall, etc.\\n        Role: Evaluation helps determine how well the model generalizes to new, unseen data.\\n\\n    Hyperparameters:\\n        Definition: Configuration settings used to control the learning process.\\n        Role: Hyperparameters are set before training and can significantly impact model performance.\\n\\n    Prediction:\\n        Definition: The output generated by the model when given new input data.\\n        Role: Predictions are used to make decisions or provide insights based on the model’s learning.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Machine learning is a subset of artificial intelligence that involves the development of algorithms and \n",
    "statistical models that enable computers to perform tasks without explicit instructions. \n",
    "Instead, these models learn patterns and make decisions based on data.\n",
    "Main Components of Machine Learning:\n",
    "\n",
    "    Data:\n",
    "        Definition: The raw information used to train and evaluate models.\n",
    "        Role: High-quality, relevant data is crucial for building effective models.\n",
    "\n",
    "    Features:\n",
    "        Definition: Individual measurable properties or characteristics used as input to the model.\n",
    "        Role: Features are used to make predictions. Feature selection and engineering are critical for model performance.\n",
    "\n",
    "    Model:\n",
    "        Definition: A mathematical representation of a real-world process.\n",
    "        Role: The model is trained on data to learn patterns and make predictions.\n",
    "\n",
    "    Algorithm:\n",
    "        Definition: A procedure or formula for solving a problem.\n",
    "        Role: Algorithms are used to train models by adjusting parameters to minimize error.\n",
    "\n",
    "    Training:\n",
    "        Definition: The process of teaching a model to make predictions by exposing it to data.\n",
    "        Role: During training, the model learns the relationship between input features and the target variable.\n",
    "\n",
    "    Evaluation:\n",
    "        Definition: Assessing the model’s performance using metrics like accuracy, precision, recall, etc.\n",
    "        Role: Evaluation helps determine how well the model generalizes to new, unseen data.\n",
    "\n",
    "    Hyperparameters:\n",
    "        Definition: Configuration settings used to control the learning process.\n",
    "        Role: Hyperparameters are set before training and can significantly impact model performance.\n",
    "\n",
    "    Prediction:\n",
    "        Definition: The output generated by the model when given new input data.\n",
    "        Role: Predictions are used to make decisions or provide insights based on the model’s learning.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19ffede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4) How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86087d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe loss value is a key metric in machine learning that quantifies how well a model’s predictions match the actual data. \\nIt plays a crucial role in evaluating and improving model performance.\\nHow Loss Value Helps:\\n\\n    Quantifies Error:\\n        The loss value measures the difference between the predicted values and the actual target values. \\n        A lower loss indicates better model performance.\\n\\n    Guides Optimization:\\n        During training, optimization algorithms (like gradient descent) use the loss value to adjust model parameters. \\n        The goal is to minimize the loss, thereby improving the model’s accuracy.\\n\\n    Model Evaluation:\\n        By comparing loss values across different models or training iterations, one can assess which model or configuration performs better.\\n\\n    Overfitting and Underfitting:\\n        A very low loss on training data but high loss on validation data may indicate overfitting, where the model learns noise instead of the underlying pattern.\\n        A high loss on both training and validation data suggests underfitting, where the model is too simple to capture the data’s complexity.\\n\\n    Choice of Loss Function:\\n        Different tasks require different loss functions. For example, mean squared error is common for regression, \\n        while cross-entropy loss is used for classification. The choice of loss function affects how the model learns.\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The loss value is a key metric in machine learning that quantifies how well a model’s predictions match the actual data. \n",
    "It plays a crucial role in evaluating and improving model performance.\n",
    "How Loss Value Helps:\n",
    "\n",
    "    Quantifies Error:\n",
    "        The loss value measures the difference between the predicted values and the actual target values. \n",
    "        A lower loss indicates better model performance.\n",
    "\n",
    "    Guides Optimization:\n",
    "        During training, optimization algorithms (like gradient descent) use the loss value to adjust model parameters. \n",
    "        The goal is to minimize the loss, thereby improving the model’s accuracy.\n",
    "\n",
    "    Model Evaluation:\n",
    "        By comparing loss values across different models or training iterations, one can assess which model or configuration performs better.\n",
    "\n",
    "    Overfitting and Underfitting:\n",
    "        A very low loss on training data but high loss on validation data may indicate overfitting, where the model learns noise instead of the underlying pattern.\n",
    "        A high loss on both training and validation data suggests underfitting, where the model is too simple to capture the data’s complexity.\n",
    "\n",
    "    Choice of Loss Function:\n",
    "        Different tasks require different loss functions. For example, mean squared error is common for regression, \n",
    "        while cross-entropy loss is used for classification. The choice of loss function affects how the model learns.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdcd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5) What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38fa3a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nContinuous Variables\\n\\n    Definition: Continuous variables are numerical variables that can take an infinite number of values within a given range. \\n    They are often measurements.\\n    Characteristics:\\n        Can be fractional or decimal.\\n        Represented on a continuous scale.\\n    Examples:\\n        Height, weight, temperature, and time.\\n    Use in Machine Learning:\\n        Often used in regression models.\\n        Require normalization or standardization for some algorithms.\\n\\nCategorical Variables\\n\\n    Definition: Categorical variables represent discrete categories or groups. They have a finite set of possible values.\\n    Characteristics:\\n        Can be nominal (no inherent order) or ordinal (ordered categories).\\n        Represented as labels or categories.\\n    Examples:\\n        Nominal: Gender, color, type of animal.\\n        Ordinal: Education level, satisfaction rating.\\n    Use in Machine Learning:\\n        Often used in classification models.\\n        Require encoding (e.g., one-hot encoding) to be used in algorithms that require numerical input.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Continuous Variables\n",
    "\n",
    "    Definition: Continuous variables are numerical variables that can take an infinite number of values within a given range. \n",
    "    They are often measurements.\n",
    "    Characteristics:\n",
    "        Can be fractional or decimal.\n",
    "        Represented on a continuous scale.\n",
    "    Examples:\n",
    "        Height, weight, temperature, and time.\n",
    "    Use in Machine Learning:\n",
    "        Often used in regression models.\n",
    "        Require normalization or standardization for some algorithms.\n",
    "\n",
    "Categorical Variables\n",
    "\n",
    "    Definition: Categorical variables represent discrete categories or groups. They have a finite set of possible values.\n",
    "    Characteristics:\n",
    "        Can be nominal (no inherent order) or ordinal (ordered categories).\n",
    "        Represented as labels or categories.\n",
    "    Examples:\n",
    "        Nominal: Gender, color, type of animal.\n",
    "        Ordinal: Education level, satisfaction rating.\n",
    "    Use in Machine Learning:\n",
    "        Often used in classification models.\n",
    "        Require encoding (e.g., one-hot encoding) to be used in algorithms that require numerical input.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c289145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6) How do we handle categorical variables in Machine Learning? What are the common techniques ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0b7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommon Techniques:\\n\\n    Label Encoding:\\n        Description: Converts each category into a unique integer.\\n        Use Case: Suitable for ordinal variables where the order matters.\\n        Limitation: Can mislead models into thinking there’s a numerical relationship between categories.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Common Techniques:\n",
    "\n",
    "    Label Encoding:\n",
    "        Description: Converts each category into a unique integer.\n",
    "        Use Case: Suitable for ordinal variables where the order matters.\n",
    "        Limitation: Can mislead models into thinking there’s a numerical relationship between categories.\n",
    "\n",
    "\"\"\"\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#data['category'] = le.fit_transform(data['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b477cbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nOne-Hot Encoding:\\n\\n    Description: Converts categories into binary vectors. Each category becomes a new column with 0s and 1s indicating presence.\\n    Use Case: Suitable for nominal variables where no order exists.\\n    Limitation: Can lead to high dimensionality with many categories.\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "One-Hot Encoding:\n",
    "\n",
    "    Description: Converts categories into binary vectors. Each category becomes a new column with 0s and 1s indicating presence.\n",
    "    Use Case: Suitable for nominal variables where no order exists.\n",
    "    Limitation: Can lead to high dimensionality with many categories.\n",
    "\n",
    "\"\"\"\n",
    "#import pandas as pd\n",
    "#data = pd.get_dummies(data, columns=['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7d47e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBinary Encoding:\\n\\n    Description: Combines label encoding and one-hot encoding. Converts categories to binary and splits digits into columns.\\n    Use Case: Useful for reducing dimensionality compared to one-hot encoding.\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary Encoding:\n",
    "\n",
    "    Description: Combines label encoding and one-hot encoding. Converts categories to binary and splits digits into columns.\n",
    "    Use Case: Useful for reducing dimensionality compared to one-hot encoding.\n",
    "\n",
    "\"\"\"\n",
    "#from category_encoders import BinaryEncoder\n",
    "\n",
    "#encoder = BinaryEncoder(cols=['category'])\n",
    "#data = encoder.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c708f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTarget Encoding:\\n\\n    Description: Replaces categories with the mean of the target variable for each category.\\n    Use Case: Useful for high-cardinality categorical variables.\\n    Limitation: Can lead to overfitting if not handled carefully.\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Target Encoding:\n",
    "\n",
    "    Description: Replaces categories with the mean of the target variable for each category.\n",
    "    Use Case: Useful for high-cardinality categorical variables.\n",
    "    Limitation: Can lead to overfitting if not handled carefully.\n",
    "\n",
    "\"\"\"\n",
    "#data['category_encoded'] = data.groupby('category')['target'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef9c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7) What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f00a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn machine learning, training and testing datasets are used to build and evaluate models. Here’s what each term means:\\nTraining Dataset\\n\\n    Purpose: Used to train the model. The model learns patterns, relationships, and features from this data.\\n    Process: During training, the model adjusts its parameters to minimize the error between its predictions and the actual outcomes.\\n    Size: Typically, the largest portion of the data is allocated for training to ensure the model learns effectively.\\n\\nTesting Dataset\\n\\n    Purpose: Used to evaluate the model’s performance. It assesses how well the model generalizes to new, unseen data.\\n    Process: After training, the model makes predictions on the testing dataset, \\n    and its performance is measured using metrics like accuracy, precision, recall, etc.\\n    Size: A smaller portion of the data is reserved for testing to provide an unbiased evaluation of the model.\\n\\nImportance of Splitting Data\\n\\n    Generalization: Ensures the model performs well on new data, not just the data it was trained on.\\n    Avoid Overfitting: Helps detect overfitting, where the model performs well on training data but poorly on unseen data.\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In machine learning, training and testing datasets are used to build and evaluate models. Here’s what each term means:\n",
    "Training Dataset\n",
    "\n",
    "    Purpose: Used to train the model. The model learns patterns, relationships, and features from this data.\n",
    "    Process: During training, the model adjusts its parameters to minimize the error between its predictions and the actual outcomes.\n",
    "    Size: Typically, the largest portion of the data is allocated for training to ensure the model learns effectively.\n",
    "\n",
    "Testing Dataset\n",
    "\n",
    "    Purpose: Used to evaluate the model’s performance. It assesses how well the model generalizes to new, unseen data.\n",
    "    Process: After training, the model makes predictions on the testing dataset, \n",
    "    and its performance is measured using metrics like accuracy, precision, recall, etc.\n",
    "    Size: A smaller portion of the data is reserved for testing to provide an unbiased evaluation of the model.\n",
    "\n",
    "Importance of Splitting Data\n",
    "\n",
    "    Generalization: Ensures the model performs well on new data, not just the data it was trained on.\n",
    "    Avoid Overfitting: Helps detect overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3180df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8) What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff2267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18321596 -1.18321596]\n",
      " [-0.50709255 -0.50709255]\n",
      " [ 0.16903085  0.16903085]\n",
      " [ 1.52127766  1.52127766]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn.preprocessing is a module in the Scikit-learn library that provides various utilities and functions for preprocessing data before it is used in machine learning models. Preprocessing is a crucial step in the machine learning pipeline, as it helps to transform raw data into a format that is more suitable for modeling.\n",
    "Key Features of sklearn.preprocessing:\n",
    "\n",
    "    Scaling and Normalization:\n",
    "        StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "        MinMaxScaler: Scales features to a specified range, typically [0, 1].\n",
    "        Normalizer: Scales individual samples to have unit norm.\n",
    "\n",
    "    Encoding Categorical Features:\n",
    "        LabelEncoder: Encodes target labels with values between 0 and n_classes-1.\n",
    "        OneHotEncoder: Converts categorical variables into a one-hot numeric array.\n",
    "\n",
    "    Binarization:\n",
    "        Binarizer: Converts numerical values into binary values based on a threshold.\n",
    "\n",
    "    Polynomial Features:\n",
    "        PolynomialFeatures: Generates polynomial and interaction features.\n",
    "\n",
    "    Imputation:\n",
    "        SimpleImputer: Replaces missing values using a specified strategy (e.g., mean, median).\n",
    "\n",
    "    Discretization:\n",
    "        KBinsDiscretizer: Discretizes continuous features into k bins.\n",
    "\n",
    "\"\"\"\n",
    "#Example\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d38958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9) What is a Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a82044a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA test set is a subset of a dataset used to evaluate the performance of a machine learning model after it has been trained. \\nIt is crucial for assessing how well the model generalizes to new, unseen data.\\nKey Characteristics of a Test Set:\\n\\n    Purpose:\\n        To provide an unbiased evaluation of a model’s performance.\\n        To simulate how the model will perform in real-world scenarios on new data.\\n\\n    Separation from Training Data:\\n        The test set is kept separate from the training data to ensure that the model’s evaluation is not influenced by the data it has already seen.\\n\\n    Evaluation Metrics:\\n        Common metrics used to evaluate the model on the test set include accuracy, precision, recall, F1-score, and mean squared error, \\n        depending on the type of problem (classification or regression).\\n\\n    Size:\\n        Typically, the test set comprises 20-30% of the total dataset, though this can vary based on the dataset size and specific requirements.\\n\\n    Role in Model Validation:\\n        The test set is used only once, after the model has been trained and validated, to provide a final assessment of its performance.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A test set is a subset of a dataset used to evaluate the performance of a machine learning model after it has been trained. \n",
    "It is crucial for assessing how well the model generalizes to new, unseen data.\n",
    "Key Characteristics of a Test Set:\n",
    "\n",
    "    Purpose:\n",
    "        To provide an unbiased evaluation of a model’s performance.\n",
    "        To simulate how the model will perform in real-world scenarios on new data.\n",
    "\n",
    "    Separation from Training Data:\n",
    "        The test set is kept separate from the training data to ensure that the model’s evaluation is not influenced by the data it has already seen.\n",
    "\n",
    "    Evaluation Metrics:\n",
    "        Common metrics used to evaluate the model on the test set include accuracy, precision, recall, F1-score, and mean squared error, \n",
    "        depending on the type of problem (classification or regression).\n",
    "\n",
    "    Size:\n",
    "        Typically, the test set comprises 20-30% of the total dataset, though this can vary based on the dataset size and specific requirements.\n",
    "\n",
    "    Role in Model Validation:\n",
    "        The test set is used only once, after the model has been trained and validated, to provide a final assessment of its performance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6ba13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10) How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323bff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[ 9 10]\n",
      " [ 5  6]\n",
      " [ 1  2]\n",
      " [ 7  8]]\n",
      "X_test: [[3 4]]\n",
      "y_train: [0 0 0 1]\n",
      "y_test: [1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In Python, data can be split into training and testing sets using the train_test_split function from the Scikit-learn library. \n",
    "This function randomly divides the dataset into two parts, \n",
    "ensuring that the model can be trained on one part and evaluated on the other.\n",
    "\"\"\"\n",
    "#Example\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train)\n",
    "print(\"X_test:\", X_test)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_test:\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3b3d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nParameters:\\n\\n    test_size: Proportion of the dataset to include in the test split (e.g., 0.2 for 20%).\\n    random_state: An integer seed for the random number generator to ensure reproducibility.\\n    train_size: Alternatively, specify the proportion for the training set.\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "\n",
    "    test_size: Proportion of the dataset to include in the test split (e.g., 0.2 for 20%).\n",
    "    random_state: An integer seed for the random number generator to ensure reproducibility.\n",
    "    train_size: Alternatively, specify the proportion for the training set.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6eef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nApproaching a machine learning problem involves several structured steps to ensure a thorough understanding and \\neffective solution. Here’s a typical approach:\\n\\n    Define the Problem:\\n        Clearly understand the problem and its requirements.\\n        Determine the type of problem (e.g., classification, regression, clustering).\\n\\n    Collect and Explore Data:\\n        Gather relevant data from various sources.\\n        Perform exploratory data analysis (EDA) to understand data distribution, patterns, and anomalies.\\n\\n    Preprocess Data:\\n        Handle missing values, outliers, and duplicates.\\n        Encode categorical variables and scale numerical features.\\n        Split data into training and testing sets.\\n\\n    Feature Engineering:\\n        Select relevant features that contribute to the model’s performance.\\n        Create new features through transformations or combinations of existing ones.\\n\\n    Select a Model:\\n        Choose appropriate algorithms based on the problem type and data characteristics.\\n        Consider models that are known to perform well for similar problems.\\n\\n    Train the Model:\\n        Use the training data to fit the model.\\n        Optimize model parameters and hyperparameters.\\n\\n    Evaluate the Model:\\n        Use the test set to assess model performance.\\n        Apply evaluation metrics relevant to the problem (e.g., accuracy, precision, recall, RMSE).\\n\\n    Tune and Improve:\\n        Fine-tune hyperparameters using techniques like grid search or random search.\\n        Experiment with different models and ensemble methods.\\n\\n    Deploy the Model:\\n        Integrate the model into a production environment.\\n        Monitor its performance and update as needed.\\n\\n    Communicate Results:\\n        Present findings and insights to stakeholders.\\n        Provide actionable recommendations based on the model’s predictions.\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Approaching a machine learning problem involves several structured steps to ensure a thorough understanding and \n",
    "effective solution. Here’s a typical approach:\n",
    "\n",
    "    Define the Problem:\n",
    "        Clearly understand the problem and its requirements.\n",
    "        Determine the type of problem (e.g., classification, regression, clustering).\n",
    "\n",
    "    Collect and Explore Data:\n",
    "        Gather relevant data from various sources.\n",
    "        Perform exploratory data analysis (EDA) to understand data distribution, patterns, and anomalies.\n",
    "\n",
    "    Preprocess Data:\n",
    "        Handle missing values, outliers, and duplicates.\n",
    "        Encode categorical variables and scale numerical features.\n",
    "        Split data into training and testing sets.\n",
    "\n",
    "    Feature Engineering:\n",
    "        Select relevant features that contribute to the model’s performance.\n",
    "        Create new features through transformations or combinations of existing ones.\n",
    "\n",
    "    Select a Model:\n",
    "        Choose appropriate algorithms based on the problem type and data characteristics.\n",
    "        Consider models that are known to perform well for similar problems.\n",
    "\n",
    "    Train the Model:\n",
    "        Use the training data to fit the model.\n",
    "        Optimize model parameters and hyperparameters.\n",
    "\n",
    "    Evaluate the Model:\n",
    "        Use the test set to assess model performance.\n",
    "        Apply evaluation metrics relevant to the problem (e.g., accuracy, precision, recall, RMSE).\n",
    "\n",
    "    Tune and Improve:\n",
    "        Fine-tune hyperparameters using techniques like grid search or random search.\n",
    "        Experiment with different models and ensemble methods.\n",
    "\n",
    "    Deploy the Model:\n",
    "        Integrate the model into a production environment.\n",
    "        Monitor its performance and update as needed.\n",
    "\n",
    "    Communicate Results:\n",
    "        Present findings and insights to stakeholders.\n",
    "        Provide actionable recommendations based on the model’s predictions.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cd76bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11) Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "515fc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExploratory Data Analysis (EDA) is a crucial step before fitting a model to the data \\nbecause it helps in understanding the dataset’s underlying structure and characteristics. \\nHere are the reasons why EDA is important:\\n\\n    Understand Data Distribution:\\n        EDA helps in visualizing the distribution of data, identifying patterns, and understanding the spread and central tendency of features.\\n\\n    Identify Outliers and Anomalies:\\n        Detecting outliers and anomalies is essential as they can skew the results and affect model performance.\\n\\n    Handle Missing Values:\\n        EDA reveals missing data, allowing for appropriate imputation or removal strategies to be applied.\\n\\n    Feature Relationships:\\n        Analyzing correlations and relationships between features can inform feature selection and engineering, \\n        improving model accuracy.\\n\\n    Detect Data Quality Issues:\\n        EDA helps identify data quality issues such as duplicates, incorrect data types, or inconsistent entries.\\n\\n    Guide Model Selection:\\n        Understanding the data’s characteristics can guide the choice of algorithms and preprocessing techniques.\\n\\n    Hypothesis Generation:\\n        EDA allows for the generation of hypotheses about the data, which can be tested and validated through modeling.\\n\\n    Reduce Dimensionality:\\n        By understanding feature importance and relationships, EDA can help in reducing dimensionality, \\n        leading to simpler and more efficient models.\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exploratory Data Analysis (EDA) is a crucial step before fitting a model to the data \n",
    "because it helps in understanding the dataset’s underlying structure and characteristics. \n",
    "Here are the reasons why EDA is important:\n",
    "\n",
    "    Understand Data Distribution:\n",
    "        EDA helps in visualizing the distribution of data, identifying patterns, and understanding the spread and central tendency of features.\n",
    "\n",
    "    Identify Outliers and Anomalies:\n",
    "        Detecting outliers and anomalies is essential as they can skew the results and affect model performance.\n",
    "\n",
    "    Handle Missing Values:\n",
    "        EDA reveals missing data, allowing for appropriate imputation or removal strategies to be applied.\n",
    "\n",
    "    Feature Relationships:\n",
    "        Analyzing correlations and relationships between features can inform feature selection and engineering, \n",
    "        improving model accuracy.\n",
    "\n",
    "    Detect Data Quality Issues:\n",
    "        EDA helps identify data quality issues such as duplicates, incorrect data types, or inconsistent entries.\n",
    "\n",
    "    Guide Model Selection:\n",
    "        Understanding the data’s characteristics can guide the choice of algorithms and preprocessing techniques.\n",
    "\n",
    "    Hypothesis Generation:\n",
    "        EDA allows for the generation of hypotheses about the data, which can be tested and validated through modeling.\n",
    "\n",
    "    Reduce Dimensionality:\n",
    "        By understanding feature importance and relationships, EDA can help in reducing dimensionality, \n",
    "        leading to simpler and more efficient models.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d09c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q14) How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ed22388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "A  1.0 -1.0  0.8\n",
      "B -1.0  1.0 -0.8\n",
      "C  0.8 -0.8  1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In Python, correlation between variables can be found using several methods, \n",
    "primarily through the use of libraries like Pandas and NumPy. Here are common approaches:\n",
    "Using Pandas\n",
    "\n",
    "    corr() Method:\n",
    "        The corr() method in Pandas computes the pairwise correlation of columns in a DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, 3, 4], 'B': [4, 3, 2, 1], 'C': [1, 3, 2, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b8e9417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing Correlation with Heatmap:\n",
    "#Use Seaborn to visualize the correlation matrix.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79061a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q15) What is causation? Explain difference between correlation and causation with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff2d9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCausation refers to a relationship where one event (the cause) directly affects another event (the effect). \\nIn other words, a change in one variable is responsible for a change in another variable.\\n    Causation:\\n        Definition: A relationship where one variable directly affects another. It implies a cause-and-effect relationship.\\n        Example: Smoking and lung cancer have a causal relationship, as smoking increases the risk of developing lung cancer.\\n\\nKey Points:\\n\\n    Directionality: Causation implies direction (cause leads to effect), while correlation does not.\\n    Confounding Variables: Correlation can be influenced by a third variable that affects both correlated variables, leading to a spurious relationship.\\n    Establishing Causation: Requires controlled experiments or additional evidence beyond statistical correlation to demonstrate a direct cause-and-effect relationship.\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Causation refers to a relationship where one event (the cause) directly affects another event (the effect). \n",
    "In other words, a change in one variable is responsible for a change in another variable.\n",
    "    Causation:\n",
    "        Definition: A relationship where one variable directly affects another. It implies a cause-and-effect relationship.\n",
    "        Example: Smoking and lung cancer have a causal relationship, as smoking increases the risk of developing lung cancer.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "    Directionality: Causation implies direction (cause leads to effect), while correlation does not.\n",
    "    Confounding Variables: Correlation can be influenced by a third variable that affects both correlated variables, leading to a spurious relationship.\n",
    "    Establishing Causation: Requires controlled experiments or additional evidence beyond statistical correlation to demonstrate a direct cause-and-effect relationship.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7579d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q16) What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33076cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMini-batch Gradient Descent:\\n\\n    Description: Combines the benefits of both batch and stochastic gradient descent by updating parameters using a small batch of data points.\\n    Example: Commonly used in deep learning frameworks like TensorFlow and PyTorch.\\n\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "An optimizer is an algorithm or method used to adjust the parameters of a model to minimize the loss function. \n",
    "Optimizers play a crucial role in training models by finding the optimal set of parameters that result in the best performance.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Types of Optimizers:\n",
    "\n",
    "    Gradient Descent:\n",
    "        Description: The most basic optimization algorithm that updates parameters by moving in the direction of the negative gradient of the loss function.\n",
    "        Example: In linear regression, gradient descent adjusts weights to minimize the mean squared error.\n",
    "\n",
    "\"\"\"\n",
    "# Pseudocode for gradient descent\n",
    "#weight = weight - learning_rate * gradient\n",
    "\n",
    "\"\"\"\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "    Description: A variant of gradient descent that updates parameters using a single data point (or a small batch) at a time, which can lead to faster convergence.\n",
    "    Example: Used in training neural networks where data is too large to fit in memory.\n",
    "\n",
    "\"\"\"\n",
    "# Pseudocode for SGD\n",
    "#for each data point:\n",
    " #   weight = weight - learning_rate * gradient(data_point)\n",
    "\n",
    "\"\"\"\n",
    "Mini-batch Gradient Descent:\n",
    "\n",
    "    Description: Combines the benefits of both batch and stochastic gradient descent by updating parameters using a small batch of data points.\n",
    "    Example: Commonly used in deep learning frameworks like TensorFlow and PyTorch.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d273e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdam (Adaptive Moment Estimation):\\n\\n    Description: Combines the ideas of momentum and RMSProp, adapting the learning rate for each parameter.\\n    Example: Widely used in training deep learning models due to its efficiency and effectiveness.\\n\\n# Pseudocode for Adam\\nm = beta1 * m + (1 - beta1) * gradient\\nv = beta2 * v + (1 - beta2) * gradient^2\\nweight = weight - learning_rate * m / (sqrt(v) + epsilon)\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Momentum:\n",
    "\n",
    "    Description: An extension of SGD that accumulates a velocity vector in the direction of the gradient to accelerate convergence.\n",
    "    Example: Helps in navigating ravines in the loss surface.\n",
    "\n",
    "# Pseudocode for momentum\n",
    "velocity = momentum * velocity - learning_rate * gradient\n",
    "weight = weight + velocity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Adam (Adaptive Moment Estimation):\n",
    "\n",
    "    Description: Combines the ideas of momentum and RMSProp, adapting the learning rate for each parameter.\n",
    "    Example: Widely used in training deep learning models due to its efficiency and effectiveness.\n",
    "\n",
    "# Pseudocode for Adam\n",
    "m = beta1 * m + (1 - beta1) * gradient\n",
    "v = beta2 * v + (1 - beta2) * gradient^2\n",
    "weight = weight - learning_rate * m / (sqrt(v) + epsilon)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eda0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q17) What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86c25acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsklearn.linear_model:\\n\\n    Linear Regression:\\n        Description: A basic linear approach for modeling the relationship between a dependent variable and one or more independent variables.\\n        Usage: Suitable for predicting continuous outcomes.\\n\\n    from sklearn.linear_model import LinearRegression\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    predictions = model.predict(X_test)\\n\\nLogistic Regression:\\n\\n    Description: A linear model for binary classification that uses the logistic function to model the probability of class membership.\\n    Usage: Suitable for binary and multiclass classification tasks.\\n\\nfrom sklearn.linear_model import LogisticRegression\\n\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\npredictions = model.predict(X_test)\\n\\nRidge Regression:\\n\\n    Description: A linear regression model with L2 regularization to prevent overfitting by penalizing large coefficients.\\n    Usage: Useful when multicollinearity is present.\\n\\nfrom sklearn.linear_model import Ridge\\n\\nmodel = Ridge(alpha=1.0)\\nmodel.fit(X_train, y_train)\\n\\nLasso Regression:\\n\\n    Description: A linear regression model with L1 regularization that can shrink some coefficients to zero, effectively performing feature selection.\\n    Usage: Useful for sparse models with many features.\\n\\nfrom sklearn.linear_model import Lasso\\n\\nmodel = Lasso(alpha=0.1)\\nmodel.fit(X_train, y_train)\\n\\nElastic Net:\\n\\n    Description: Combines L1 and L2 regularization, balancing between Ridge and Lasso.\\n    Usage: Useful when multiple features are correlated.\\n\\nfrom sklearn.linear_model import ElasticNet\\n\\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.5)\\nmodel.fit(X_train, y_train)\\n\\nPerceptron:\\n\\n    Description: A simple linear binary classifier.\\n    Usage: Suitable for binary classification tasks.\\n\\nfrom sklearn.linear_model import Perceptron\\n\\nmodel = Perceptron()\\nmodel.fit(X_train, y_train)\\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn.linear_model:\n",
    "\n",
    "    Linear Regression:\n",
    "        Description: A basic linear approach for modeling the relationship between a dependent variable and one or more independent variables.\n",
    "        Usage: Suitable for predicting continuous outcomes.\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "    Description: A linear model for binary classification that uses the logistic function to model the probability of class membership.\n",
    "    Usage: Suitable for binary and multiclass classification tasks.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "    Description: A linear regression model with L2 regularization to prevent overfitting by penalizing large coefficients.\n",
    "    Usage: Useful when multicollinearity is present.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "    Description: A linear regression model with L1 regularization that can shrink some coefficients to zero, effectively performing feature selection.\n",
    "    Usage: Useful for sparse models with many features.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Elastic Net:\n",
    "\n",
    "    Description: Combines L1 and L2 regularization, balancing between Ridge and Lasso.\n",
    "    Usage: Useful when multiple features are correlated.\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Perceptron:\n",
    "\n",
    "    Description: A simple linear binary classifier.\n",
    "    Usage: Suitable for binary classification tasks.\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "model = Perceptron()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcb92211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18) What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfb92a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe model.fit() method in Scikit-learn is used to train a machine learning model on a given dataset. It adjusts the model’s parameters based on the input data to minimize the error between the predicted and actual outcomes.\\nWhat model.fit() Does:\\n\\n    Training the Model:\\n        It uses the provided data to learn the relationship between the input features and the target variable.\\n        The model’s parameters are optimized to best fit the training data.\\n\\n    Adjusting Parameters:\\n        For linear models, this involves finding the best-fitting line or hyperplane.\\n        For other models, it involves optimizing the model’s internal parameters to minimize the loss function.\\n\\nRequired Arguments:\\n\\n    X (Features):\\n        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features.\\n        Each row represents a sample, and each column represents a feature.\\n\\n    y (Target):\\n        A 1D array-like structure containing the target variable or labels.\\n        For regression, y contains continuous values; for classification, it contains class labels.\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model.fit() method in Scikit-learn is used to train a machine learning model on a given dataset. It adjusts the model’s parameters based on the input data to minimize the error between the predicted and actual outcomes.\n",
    "What model.fit() Does:\n",
    "\n",
    "    Training the Model:\n",
    "        It uses the provided data to learn the relationship between the input features and the target variable.\n",
    "        The model’s parameters are optimized to best fit the training data.\n",
    "\n",
    "    Adjusting Parameters:\n",
    "        For linear models, this involves finding the best-fitting line or hyperplane.\n",
    "        For other models, it involves optimizing the model’s internal parameters to minimize the loss function.\n",
    "\n",
    "Required Arguments:\n",
    "\n",
    "    X (Features):\n",
    "        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features.\n",
    "        Each row represents a sample, and each column represents a feature.\n",
    "\n",
    "    y (Target):\n",
    "        A 1D array-like structure containing the target variable or labels.\n",
    "        For regression, y contains continuous values; for classification, it contains class labels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c59bf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X_train = [[1, 2], [3, 4], [5, 6]]\n",
    "y_train = [3, 7, 11]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4137792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19) What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b35c968f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe model.predict() method in Scikit-learn is used to make predictions based on the input data using a trained machine learning model. \\nAfter a model has been trained with model.fit(), predict() applies the learned patterns to new data to generate predictions.\\nWhat model.predict() Does:\\n\\n    Generates Predictions:\\n        Uses the trained model to predict the target variable for new, unseen data.\\n        Applies the model’s learned parameters to the input features to produce output values.\\n\\n    Output:\\n        For regression models, it returns continuous values.\\n        For classification models, it returns class labels.\\n\\nRequired Argument:\\n\\n    X (Features):\\n        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features for which predictions are to be made.\\n        Each row represents a sample, and each column represents a feature.\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model.predict() method in Scikit-learn is used to make predictions based on the input data using a trained machine learning model. \n",
    "After a model has been trained with model.fit(), predict() applies the learned patterns to new data to generate predictions.\n",
    "What model.predict() Does:\n",
    "\n",
    "    Generates Predictions:\n",
    "        Uses the trained model to predict the target variable for new, unseen data.\n",
    "        Applies the model’s learned parameters to the input features to produce output values.\n",
    "\n",
    "    Output:\n",
    "        For regression models, it returns continuous values.\n",
    "        For classification models, it returns class labels.\n",
    "\n",
    "Required Argument:\n",
    "\n",
    "    X (Features):\n",
    "        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features for which predictions are to be made.\n",
    "        Each row represents a sample, and each column represents a feature.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d642a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 9.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X_train = [[1, 2], [3, 4], [5, 6]]\n",
    "y_train = [3, 7, 11]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# New data for prediction\n",
    "X_test = [[2, 3], [4, 5]]\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63455a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q21) What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1016cee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMin-Max Scaling (Normalization):\\n\\n    Description: Scales the data to a fixed range, usually [0, 1].\\n   \\n\\n    Result: Features are scaled to a specified range.\\n\\nMaxAbs Scaling:\\n\\n    Description: Scales each feature by its maximum absolute value.\\n    Result: Features are in the range [-1, 1].\\n\\nRobust Scaling:\\n\\n    Description: Uses the median and interquartile range for scaling, making it robust to outliers.\\n    Result: Features are scaled based on percentiles.\\n\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature scaling is a preprocessing technique used to standardize the range of independent variables or features in a dataset. \n",
    "It ensures that each feature contributes equally to the distance calculations and model training, \n",
    "which is crucial for many machine learning algorithms.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Standardization (Z-score Normalization):\n",
    "\n",
    "    Description: Centers the data by subtracting the mean and scales it by dividing by the standard deviation.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Min-Max Scaling (Normalization):\n",
    "\n",
    "    Description: Scales the data to a fixed range, usually [0, 1].\n",
    "   \n",
    "\n",
    "    Result: Features are scaled to a specified range.\n",
    "\n",
    "MaxAbs Scaling:\n",
    "\n",
    "    Description: Scales each feature by its maximum absolute value.\n",
    "    Result: Features are in the range [-1, 1].\n",
    "\n",
    "Robust Scaling:\n",
    "\n",
    "    Description: Uses the median and interquartile range for scaling, making it robust to outliers.\n",
    "    Result: Features are scaled based on percentiles.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8111f52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n    Improves Convergence:\\n        Algorithms like gradient descent converge faster with scaled features, as they ensure a more uniform scale.\\n\\n    Enhances Model Performance:\\n        Models sensitive to feature magnitudes, such as SVMs and k-NN, perform better with scaled data.\\n\\n    Prevents Dominance:\\n        Prevents features with larger ranges from dominating the model’s learning process.\\n\\n    Improves Interpretability:\\n        Scaled features make it easier to compare the importance of different features.\\n\\n    Reduces Bias:\\n        Ensures that the model treats all features equally, reducing bias towards features with larger scales.\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "    Improves Convergence:\n",
    "        Algorithms like gradient descent converge faster with scaled features, as they ensure a more uniform scale.\n",
    "\n",
    "    Enhances Model Performance:\n",
    "        Models sensitive to feature magnitudes, such as SVMs and k-NN, perform better with scaled data.\n",
    "\n",
    "    Prevents Dominance:\n",
    "        Prevents features with larger ranges from dominating the model’s learning process.\n",
    "\n",
    "    Improves Interpretability:\n",
    "        Scaled features make it easier to compare the importance of different features.\n",
    "\n",
    "    Reduces Bias:\n",
    "        Ensures that the model treats all features equally, reducing bias towards features with larger scales.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16c2cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q22) How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4997d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Standardization (Z-score Normalization)\n",
    "\n",
    "    StandardScaler: Centers the data by subtracting the mean and scales it by dividing by the standard deviation.\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52184b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Min-Max Scaling (Normalization)\n",
    "\n",
    "    MinMaxScaler: Scales the data to a fixed range, usually [0, 1].\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbf30b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2        -0.33333333]\n",
      " [ 0.6        -0.66666667]\n",
      " [ 1.         -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxAbs Scaling\n",
    "\n",
    "    MaxAbsScaler: Scales each feature by its maximum absolute value.\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, -2], [3, -4], [5, -6]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3df8814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust Scaling\n",
    "\n",
    "    RobustScaler: Uses the median and interquartile range for scaling, making it robust to outliers.\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b21f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q25) Explain data encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bd234f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLabel Encoding:\\n\\n    Description: Converts each category into a unique integer.\\n    Use Case: Suitable for ordinal variables where the order matters.\\n    Limitation: Can mislead models into thinking there’s a numerical relationship between categories.\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data encoding is the process of converting categorical data into a numerical format that can be used by machine learning algorithms.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Label Encoding:\n",
    "\n",
    "    Description: Converts each category into a unique integer.\n",
    "    Use Case: Suitable for ordinal variables where the order matters.\n",
    "    Limitation: Can mislead models into thinking there’s a numerical relationship between categories.\n",
    "\n",
    "\"\"\"\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#data['category'] = le.fit_transform(data['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0b40c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne-Hot Encoding:\\n\\n    Description: Converts categories into binary vectors. Each category becomes a new column with 0s and 1s indicating presence.\\n    Use Case: Suitable for nominal variables where no order exists.\\n    Limitation: Can lead to high dimensionality with many categories.\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "One-Hot Encoding:\n",
    "\n",
    "    Description: Converts categories into binary vectors. Each category becomes a new column with 0s and 1s indicating presence.\n",
    "    Use Case: Suitable for nominal variables where no order exists.\n",
    "    Limitation: Can lead to high dimensionality with many categories.\n",
    "\n",
    "\"\"\"\n",
    "#import pandas as pd\n",
    "#data = pd.get_dummies(data, columns=['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34eb74df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBinary Encoding:\\n\\n    Description: Combines label encoding and one-hot encoding. Converts categories to binary and splits digits into columns.\\n    Use Case: Useful for reducing dimensionality compared to one-hot encoding.\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary Encoding:\n",
    "\n",
    "    Description: Combines label encoding and one-hot encoding. Converts categories to binary and splits digits into columns.\n",
    "    Use Case: Useful for reducing dimensionality compared to one-hot encoding.\n",
    "\n",
    "\"\"\"\n",
    "#from category_encoders import BinaryEncoder\n",
    "\n",
    "#encoder = BinaryEncoder(cols=['category'])\n",
    "#data = encoder.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fbebad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTarget Encoding:\\n\\n    Description: Replaces categories with the mean of the target variable for each category.\\n    Use Case: Useful for high-cardinality categorical variables.\\n    Limitation: Can lead to overfitting if not handled carefully.\\n\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Target Encoding:\n",
    "\n",
    "    Description: Replaces categories with the mean of the target variable for each category.\n",
    "    Use Case: Useful for high-cardinality categorical variables.\n",
    "    Limitation: Can lead to overfitting if not handled carefully.\n",
    "\n",
    "\"\"\"\n",
    "#data['category_encoded'] = data.groupby('category')['target'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca5feb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFrequency Encoding:\\n\\n    Description: Replaces categories with their frequency in the dataset.\\n    Use Case: Useful for capturing the importance of categories based on their occurrence.\\n\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Frequency Encoding:\n",
    "\n",
    "    Description: Replaces categories with their frequency in the dataset.\n",
    "    Use Case: Useful for capturing the importance of categories based on their occurrence.\n",
    "\n",
    "\"\"\"\n",
    "#data['category_encoded'] = data['category'].map(data['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2eb07bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    Model Compatibility: Ensures that categorical data can be used with algorithms that require numerical input.\\n    Improves Model Performance: Proper encoding can lead to better model accuracy and efficiency.\\n    Handles Categorical Data: Allows models to learn from categorical features effectively.\\n\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Model Compatibility: Ensures that categorical data can be used with algorithms that require numerical input.\n",
    "    Improves Model Performance: Proper encoding can lead to better model accuracy and efficiency.\n",
    "    Handles Categorical Data: Allows models to learn from categorical features effectively.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25f5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
